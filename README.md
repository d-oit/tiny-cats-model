# tiny-cats-model

[![Python Version](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![CI](https://github.com/d-oit/tiny-cats-model/actions/workflows/ci.yml/badge.svg)](https://github.com/d-oit/tiny-cats-model/actions/workflows/ci.yml)
[![Code Quality: Ruff](https://img.shields.io/badge/code%20quality-ruff-ff0000)](https://github.com/astral-sh/ruff)
[![Type Check: mypy](https://img.shields.io/badge/type%20check-mypy-blue)](https://github.com/python/mypy)
[![HuggingFace Model](https://img.shields.io/badge/ðŸ¤—-Model-yellow)](https://huggingface.co/d4oit/tiny-cats-model)

A cats classifier and generator built on PyTorch with ResNet-18 and TinyDiT, following 2026 best practices for AI-agent-friendly repositories.

## Features

- **Classification**: ResNet-18 fine-tuned for cat breed classification (13 breeds)
- **Generation**: TinyDiT diffusion model for conditional cat image generation
- **Interactive Tutorials**: 3 Jupyter notebooks with Google Colab support
- **Automated Deployment**: CI/CD pipeline with automated HuggingFace uploads
- **Comprehensive Testing**: 215+ E2E tests covering all user journeys
- **ONNX Export**: Quantized models for web deployment (11MB classifier, 33MB generator)

## Quick Links

- ðŸ“š [Tutorial Notebooks](notebooks/README.md) - Interactive guides with Colab
- ðŸ¤— [HuggingFace Model](https://huggingface.co/d4oit/tiny-cats-model) - Download models
- ðŸ“– [Documentation](docs/) - Setup guides and ADRs
- ðŸ§ª [E2E Tests](tests/e2e/) - Playwright test suite

## Requirements

- Python 3.10+
- PyTorch 2.0+
- CUDA (optional, for GPU training)

## Quickstart

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Download & prepare dataset
bash data/download.sh

# 3. Train the model
python src/train.py data/cats

# 4. Evaluate
python src/eval.py
```

## Project Structure

```
tiny-cats-model/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py           # Training entrypoint
â”‚   â”œâ”€â”€ eval.py            # Evaluation script
â”‚   â”œâ”€â”€ model.py           # Model definition
â”‚   â”œâ”€â”€ dataset.py         # DataLoader factory
â”‚   â””â”€â”€ export_onnx.py     # ONNX export
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_dataset.py    # Unit tests
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cats/              # Dataset (gitignored)
â”‚   â””â”€â”€ download.sh        # Dataset download script
â”œâ”€â”€ .agents/skills/        # Agent automation skills
â”‚   â”œâ”€â”€ cli-usage/         # Training & evaluation commands
â”‚   â”œâ”€â”€ testing-workflow/  # CI verification
â”‚   â”œâ”€â”€ code-quality/      # Linting & formatting
â”‚   â”œâ”€â”€ gh-actions/       # CI/CD debugging
â”‚   â”œâ”€â”€ git-workflow/      # Branch & PR management
â”‚   â”œâ”€â”€ security/          # Secrets handling
â”‚   â””â”€â”€ model-training/   # GPU training
â”œâ”€â”€ .github/workflows/    # CI/CD pipelines
â”œâ”€â”€ plans/                # Architecture decision records
â”œâ”€â”€ AGENTS.md             # AI agent guidance
â”œâ”€â”€ CLAUDE.md             # Claude CLI reference
â”œâ”€â”€ modal.yml             # Modal GPU config
â””â”€â”€ requirements.txt      # Dependencies
```

## Training Options

```bash
# Default (10 epochs, resnet18)
python src/train.py data/cats

# Custom training
python src/train.py data/cats \
  --epochs 20 \
  --batch-size 64 \
  --lr 0.0001 \
  --backbone resnet34 \
  --output my_model.pt

# Train without pretrained weights
python src/train.py data/cats --no-pretrained
```

## Modal Training (GPU)

```bash
export MODAL_TOKEN_ID=your_token_id
export MODAL_TOKEN_SECRET=your_token_secret
modal run src/train.py
```

> **Security**: Never commit secrets. Use environment variables or GitHub Secrets.

## Development

```bash
# Run tests
pytest tests/ -v

# Lint code (auto-fix)
ruff check . --fix

# Format code
ruff format .

# Type check
mypy .

# Full verification
bash scripts/quality-gate.sh
```

## Dataset

Default: Oxford IIIT Pet Dataset (cats subset). The `data/download.sh` script downloads and prepares the dataset. Replace the URL with your own source if needed.

## License

MIT
