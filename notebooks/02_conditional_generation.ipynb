{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Conditional Cat Image Generation\n",
    "\n",
    "Generate realistic cat images of specific breeds using our pre-trained TinyDiT diffusion model.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- Load generator model from HuggingFace\n",
    "- Generate cat images by breed\n",
    "- Adjust classifier-free guidance (CFG) for variety\n",
    "- Understand generation trade-offs\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Important:** This notebook works best with GPU acceleration. On Colab, use Runtime ‚Üí Change runtime type ‚Üí GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch huggingface_hub pillow numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"‚úÖ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"‚ö†Ô∏è  Using CPU (slower, but works)\")\n",
    "    print(\"   For faster generation, use GPU runtime (Colab: Runtime ‚Üí Change runtime type ‚Üí GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Generator Model\n",
    "\n",
    "We'll load the PyTorch checkpoint from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path for model imports\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Download model\n",
    "print(\"Downloading generator model...\")\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"d4oit/tiny-cats-model\",\n",
    "    filename=\"generator/model.pt\"\n",
    ")\n",
    "print(f\"Model downloaded to: {model_path}\")\n",
    "\n",
    "# Import model architecture\n",
    "from src.dit import tinydit_128\n",
    "\n",
    "# Load checkpoint\n",
    "print(\"Loading model weights...\")\n",
    "checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "\n",
    "# Handle different checkpoint formats\n",
    "if \"model_state_dict\" in checkpoint:\n",
    "    model_state = checkpoint[\"model_state_dict\"]\n",
    "elif \"state_dict\" in checkpoint:\n",
    "    model_state = checkpoint[\"state_dict\"]\n",
    "else:\n",
    "    model_state = checkpoint\n",
    "\n",
    "# Initialize model\n",
    "model = tinydit_128(\n",
    "    image_size=128,\n",
    "    patch_size=16,\n",
    "    embed_dim=384,\n",
    "    depth=12,\n",
    "    num_heads=6,\n",
    "    num_classes=13  # 12 breeds + other\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.load_state_dict(model_state, strict=False)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Breed Names and Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cat breed names (must match training data)\n",
    "BREED_NAMES = [\n",
    "    \"Abyssinian\",\n",
    "    \"Bengal\",\n",
    "    \"Birman\",\n",
    "    \"Bombay\",\n",
    "    \"British Shorthair\",\n",
    "    \"Egyptian Mau\",\n",
    "    \"Maine Coon\",\n",
    "    \"Persian\",\n",
    "    \"Ragdoll\",\n",
    "    \"Russian Blue\",\n",
    "    \"Siamese\",\n",
    "    \"Sphynx\",\n",
    "    \"Other\"\n",
    "]\n",
    "\n",
    "print(\"Supported breeds:\")\n",
    "for i, breed in enumerate(BREED_NAMES):\n",
    "    print(f\"  {i}: {breed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the sampling function for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def sample_t(batch_size: int, device: str) -> torch.Tensor:\n",
    "    \"\"\"Sample timesteps uniformly from [0, 1].\"\"\"\n",
    "    return torch.rand(batch_size, device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: torch.nn.Module,\n",
    "    breed_index: int,\n",
    "    num_steps: int = 50,\n",
    "    cfg_scale: float = 1.5,\n",
    "    batch_size: int = 1,\n",
    "    device: str = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate cat images using flow matching with CFG.\n",
    "    \n",
    "    Args:\n",
    "        model: TinyDiT model\n",
    "        breed_index: Index of breed to generate (0-12)\n",
    "        num_steps: Number of ODE integration steps\n",
    "        cfg_scale: Classifier-free guidance scale\n",
    "        batch_size: Number of images to generate\n",
    "        device: Device to run on\n",
    "    \n",
    "    Returns:\n",
    "        Generated images (batch_size, 3, 128, 128) in [-1, 1] range\n",
    "    \"\"\"\n",
    "    # Sample initial noise\n",
    "    z = torch.randn(batch_size, 3, 128, 128, device=device)\n",
    "    \n",
    "    # Create one-hot breed vector\n",
    "    y = torch.zeros(batch_size, 13, device=device)\n",
    "    y[:, breed_index] = 1\n",
    "    \n",
    "    # Null condition for CFG (unconditional)\n",
    "    y_null = torch.zeros(batch_size, 13, device=device)\n",
    "    \n",
    "    # Euler integration\n",
    "    dt = 1.0 / num_steps\n",
    "    x = z.clone()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        t = torch.full((batch_size,), step * dt, device=device)\n",
    "        \n",
    "        # Conditional prediction\n",
    "        pred_cond = model(x, t, y)\n",
    "        \n",
    "        # Unconditional prediction (for CFG)\n",
    "        if cfg_scale != 1.0:\n",
    "            pred_uncond = model(x, t, y_null)\n",
    "            # Apply CFG: v = v_uncond + cfg_scale * (v_cond - v_uncond)\n",
    "            pred = pred_uncond + cfg_scale * (pred_cond - pred_uncond)\n",
    "        else:\n",
    "            pred = pred_cond\n",
    "        \n",
    "        # Euler step: x_{t+1} = x_t + v * dt\n",
    "        x = x + pred * dt\n",
    "    \n",
    "    return x\n",
    "\n",
    "def tensor_to_image(tensor: torch.Tensor) -> Image.Image:\n",
    "    \"\"\"Convert tensor to PIL image.\"\"\"\n",
    "    # Convert from [-1, 1] to [0, 255]\n",
    "    image = ((tensor + 1) / 2 * 255).clip(0, 255).to(torch.uint8)\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "    return Image.fromarray(image)\n",
    "\n",
    "print(\"Generation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Your First Cat Image\n",
    "\n",
    "Let's generate an Abyssinian cat (breed index 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Generate Abyssinian\n",
    "breed_index = 0  # Abyssinian\n",
    "breed_name = BREED_NAMES[breed_index]\n",
    "\n",
    "print(f\"Generating {breed_name}...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = generate(\n",
    "        model,\n",
    "        breed_index=breed_index,\n",
    "        num_steps=50,\n",
    "        cfg_scale=1.5,\n",
    "        batch_size=1,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Generation complete in {elapsed:.2f} seconds!\")\n",
    "\n",
    "# Convert to image and display\n",
    "image = tensor_to_image(generated[0])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Generated {breed_name}\", fontsize=16, fontweight='bold')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save image\n",
    "output_path = f\"generated_{breed_name.lower().replace(' ', '_')}.png\"\n",
    "image.save(output_path)\n",
    "print(f\"Image saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate All Breeds\n",
    "\n",
    "Let's generate one image for each of the 13 breeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating all 13 breeds...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create figure for grid\n",
    "fig, axes = plt.subplots(3, 5, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "generated_images = []\n",
    "\n",
    "for i, breed_name in enumerate(BREED_NAMES):\n",
    "    if i >= len(axes):\n",
    "        break\n",
    "    \n",
    "    print(f\"  [{i+1}/13] Generating {breed_name}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = generate(\n",
    "            model,\n",
    "            breed_index=i,\n",
    "            num_steps=50,\n",
    "            cfg_scale=1.5,\n",
    "            batch_size=1,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    image = tensor_to_image(generated[0])\n",
    "    generated_images.append(image)\n",
    "    \n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(breed_name, fontsize=12, fontweight='bold')\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Generated all breeds in {elapsed:.2f} seconds!\")\n",
    "print(f\"   Average: {elapsed/13:.2f} seconds per breed\")\n",
    "\n",
    "plt.suptitle(\"TinyDiT Generated Cat Breeds\", fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save grid\n",
    "fig.savefig(\"all_breeds_grid.png\", dpi=150, bbox_inches='tight')\n",
    "print(f\"Grid saved to: all_breeds_grid.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Experiment with CFG Scale\n",
    "\n",
    "Classifier-Free Guidance (CFG) controls the trade-off between diversity and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different CFG values\n",
    "cfg_values = [0.5, 1.0, 1.5, 2.0, 3.0]\n",
    "breed_index = 0  # Abyssinian\n",
    "\n",
    "print(f\"Testing CFG scales: {cfg_values}\")\n",
    "print(f\"Breed: {BREED_NAMES[breed_index]}\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(1, len(cfg_values), figsize=(20, 4))\n",
    "\n",
    "for idx, cfg in enumerate(cfg_values):\n",
    "    print(f\"  Generating with CFG={cfg}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = generate(\n",
    "            model,\n",
    "            breed_index=breed_index,\n",
    "            num_steps=50,\n",
    "            cfg_scale=cfg,\n",
    "            batch_size=1,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    image = tensor_to_image(generated[0])\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].set_title(f\"CFG = {cfg}\", fontsize=14)\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"CFG Scale Comparison - {BREED_NAMES[breed_index]}\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìù Observation:\")\n",
    "print(\"   - Low CFG (0.5-1.0): More diverse but less coherent\")\n",
    "print(\"   - Medium CFG (1.5-2.0): Good balance (recommended)\")\n",
    "print(\"   - High CFG (3.0+): Sharper but less diverse, possible artifacts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Multiple Variations\n",
    "\n",
    "Generate multiple images of the same breed to see variety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breed_index = 5  # Egyptian Mau\n",
    "num_variations = 8\n",
    "\n",
    "print(f\"Generating {num_variations} variations of {BREED_NAMES[breed_index]}...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_variations):\n",
    "    with torch.no_grad():\n",
    "        generated = generate(\n",
    "            model,\n",
    "            breed_index=breed_index,\n",
    "            num_steps=50,\n",
    "            cfg_scale=1.5,\n",
    "            batch_size=1,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    image = tensor_to_image(generated[0])\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(f\"Variation {i+1}\", fontsize=12)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"{BREED_NAMES[breed_index]} - Multiple Variations\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Each variation is unique due to random noise initialization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Batch Generation\n",
    "\n",
    "Generate multiple images in parallel for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "breed_index = 2  # Birman\n",
    "\n",
    "print(f\"Generating {batch_size} images in batch...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_batch = generate(\n",
    "        model,\n",
    "        breed_index=breed_index,\n",
    "        num_steps=50,\n",
    "        cfg_scale=1.5,\n",
    "        batch_size=batch_size,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Batch generation complete in {elapsed:.2f} seconds!\")\n",
    "print(f\"Per-image time: {elapsed/batch_size:.2f} seconds\")\n",
    "\n",
    "# Display batch\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(batch_size):\n",
    "    image = tensor_to_image(generated_batch[i])\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].set_title(f\"#{i+1}\", fontsize=12)\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Batch Generation - {BREED_NAMES[breed_index]}\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Speed vs Quality Trade-off\n",
    "\n",
    "Fewer steps = faster generation but lower quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_configs = [\n",
    "    {\"steps\": 10, \"label\": \"Fast (10 steps)\"},\n",
    "    {\"steps\": 25, \"label\": \"Balanced (25 steps)\"},\n",
    "    {\"steps\": 50, \"label\": \"Quality (50 steps)\"},\n",
    "    {\"steps\": 100, \"label\": \"Best (100 steps)\"},\n",
    "]\n",
    "\n",
    "breed_index = 10  # Siamese\n",
    "\n",
    "fig, axes = plt.subplots(1, len(step_configs), figsize=(20, 4))\n",
    "\n",
    "for idx, config in enumerate(step_configs):\n",
    "    print(f\"Generating with {config['steps']} steps...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        generated = generate(\n",
    "            model,\n",
    "            breed_index=breed_index,\n",
    "            num_steps=config[\"steps\"],\n",
    "            cfg_scale=1.5,\n",
    "            batch_size=1,\n",
    "            device=device\n",
    "        )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    image = tensor_to_image(generated[0])\n",
    "    \n",
    "    axes[idx].imshow(image)\n",
    "    axes[idx].set_title(f\"{config['label']}\\n{elapsed:.2f}s\", fontsize=12)\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Step Count Comparison - {BREED_NAMES[breed_index]}\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìù Recommendation:\")\n",
    "print(\"   - 10-25 steps: Quick previews, drafts\")\n",
    "print(\"   - 50 steps: Good quality (default)\")\n",
    "print(\"   - 100 steps: Maximum quality, final outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Save Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"generated_cats\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Saving generated images to {output_dir}/...\")\n",
    "\n",
    "# Generate and save one image per breed\n",
    "for i, breed_name in enumerate(BREED_NAMES):\n",
    "    with torch.no_grad():\n",
    "        generated = generate(\n",
    "            model,\n",
    "            breed_index=i,\n",
    "            num_steps=50,\n",
    "            cfg_scale=1.5,\n",
    "            batch_size=1,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    image = tensor_to_image(generated[0])\n",
    "    filename = f\"{breed_name.lower().replace(' ', '_')}.png\"\n",
    "    image.save(os.path.join(output_dir, filename))\n",
    "    print(f\"  ‚úì {breed_name}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Saved {len(BREED_NAMES)} images to {output_dir}/\")\n",
    "print(f\"   Files: {os.listdir(output_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tips\n",
    "\n",
    "### 1. Use GPU\n",
    "GPU acceleration provides 10-50x speedup for diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current device\n",
    "print(f\"Current device: {device}\")\n",
    "\n",
    "# On Colab: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "# Then re-run all cells from the beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reduce Steps for Faster Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick preview (5 steps)\n",
    "with torch.no_grad():\n",
    "    quick = generate(model, breed_index=0, num_steps=5, cfg_scale=1.5, device=device)\n",
    "    \n",
    "# High quality (100 steps)\n",
    "with torch.no_grad():\n",
    "    quality = generate(model, breed_index=0, num_steps=100, cfg_scale=1.5, device=device)\n",
    "\n",
    "print(\"Quick: ~1 second, Quality: ~10 seconds (on GPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Batch Generation\n",
    "Generate multiple images in parallel for better GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 8 images at once\n",
    "with torch.no_grad():\n",
    "    batch = generate(model, breed_index=0, num_steps=50, cfg_scale=1.5, batch_size=8, device=device)\n",
    "\n",
    "print(f\"Generated {batch.shape[0]} images in one batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Issues & Solutions\n",
    "\n",
    "### Issue 1: \"CUDA out of memory\"\n",
    "**Solution:** Reduce batch_size to 1 or use CPU.\n",
    "\n",
    "### Issue 2: \"Slow generation on CPU\"\n",
    "**Solution:** Use fewer steps (10-25) or switch to GPU.\n",
    "\n",
    "### Issue 3: \"Poor image quality\"\n",
    "**Solution:** Increase num_steps to 50-100 or adjust CFG scale.\n",
    "\n",
    "### Issue 4: \"All images look the same\"\n",
    "**Solution:** Each generation uses random noise - they should be different. If not, check random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ You've learned how to:\n",
    "- Load a diffusion generator from HuggingFace\n",
    "- Generate cat images by breed\n",
    "- Adjust CFG scale for quality/variety trade-off\n",
    "- Optimize generation speed vs quality\n",
    "- Batch generate for efficiency\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try [Notebook 03: Training & Fine-Tuning](03_training_fine_tuning.ipynb)\n",
    "- Read about the model architecture in [ADR-017](../plans/ADR-017-tinydit-training-infrastructure.md)\n",
    "- Learn about flow matching in [ADR-008](../plans/ADR-008-adapt-tiny-models-architecture-for-cats-classifier-with-web-frontend.md)\n",
    "- Check out the [model repository](https://huggingface.co/d4oit/tiny-cats-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Model: https://huggingface.co/d4oit/tiny-cats-model\n",
    "- DiT Paper: https://arxiv.org/abs/2212.09748\n",
    "- Flow Matching: https://arxiv.org/abs/2210.02747\n",
    "- Classifier-Free Guidance: https://arxiv.org/abs/2207.12598"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
